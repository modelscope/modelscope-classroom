{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf620d8-edad-4b3d-964d-35e5242d2062",
   "metadata": {},
   "source": [
    "# 🤖 在Jupyter Notebook中玩转Qwen3-VL视觉大模型：从部署到对话全指南"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028dc89-54c6-456e-9789-2b53f346cda9",
   "metadata": {},
   "source": [
    "随着大模型的应用越来越广泛，相信大家已经不再满足于只能与助手聊天的单一功能，对大模型处理图像的需求也日渐变多。本教程将手把手带你，在熟悉的Jupyter Notebook环境中，通过llama.cpp轻松部署强大的Qwen3-VL多模态模型。告别复杂的命令行和云服务依赖，只需一步步跟随，即可在本地构建一个支持图像理解与智能对话的私人助手。\n",
    "核心优势：\n",
    "- **完全本地化**：模型在本地运行，无需网络，隐私无忧。\n",
    "\n",
    "- **多模态能力**：不仅会聊天，更能理解你发送的图片内容。\n",
    "\n",
    "- **Notebook友好**：所有操作均在Jupyter单元格内完成，交互直观。\n",
    "\n",
    "- **免费开源**：依托llama.cpp和ModelScope开源社区。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa457f47-2cbe-4301-9a71-02892cc671d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🎯 极速预览：成功部署“三步曲”\n",
    "如果你喜欢直入主题，以下是已验证的成功路径摘要：（---在终端中执行命令---）\n",
    "### 第1步：从ModelScope下载Qwen3模型文件\n",
    "```python\n",
    "pip install modelscope --upgrade\n",
    "modelscope download --model 'Qwen/Qwen3-VL-2B-Instruct-GGUF' Qwen3VL-2B-Instruct-Q4_K_M.gguf mmproj-Qwen3VL-2B-Instruct-F16.gguf (下载到默认cache地址)\n",
    "```\n",
    "\n",
    "### 第2步：获取并编译模型引擎 (llama.cpp)\n",
    "```python\n",
    "git clone https://github.com/ggml-org/llama.cpp\n",
    "cd llama.cpp\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)\n",
    "```\n",
    "\n",
    "### 第3步：启动本地服务器（替换实际路径）\n",
    "```\n",
    "  ./build/bin/llama-server \\\n",
    "  -m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "  --mmproj /your/path/to/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "  -c 4096 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "  ```\n",
    "\n",
    "### 第4步：启动模型交互界面（可选）\n",
    "```\n",
    "./build/bin/llama-cli \\\n",
    "-m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "--mmproj /your/path/to/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "--jinja \\\n",
    "--color auto\n",
    "```\n",
    "\n",
    "### 注意----第一步模型下载和第三步的启动server可以用一步来搞定，即：\n",
    "但是要注意加上`--mmproj`参数来指定投影文件，启用多模态\n",
    "```\n",
    "export MODEL_ENDPOINT=https://www.modelscope.cn/\n",
    "./build/bin/llama-server -hf Qwen/Qwen3-VL-2B-Instruct-GGUF --jinja -ngl 99 -fa auto -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift\n",
    "```\n",
    "类似的，第一步和第四步也可以合并为一步：\n",
    "```\n",
    "export MODEL_ENDPOINT=https://www.modelscope.cn/\n",
    "./build/bin/llama-cli -hf Qwen/Qwen3-VL-2B-Instruct-GGUF --jinja --color auto -ngl 99 -fa auto -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift\n",
    "```\n",
    "（这样通过设置`MODEL_ENDPOINT`环境变量，可以让程序从ModelScope下载模型，下载完成之后直接启动）\\\n",
    "下面，我们将详细展开每一步在Notebook中的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81652a89-086d-4422-8995-793256a58b2f",
   "metadata": {},
   "source": [
    "## 📦 第一章：战前准备（环境与模型）\n",
    "\n",
    "### 1.1 认识llama.cpp\n",
    "开始之前，我们先简要介绍一下 llama.cpp，说明你可以从中获得什么，以及为什么我们要强调“使用” llama.cpp。本质上，llama.cpp 是一个独立的生态系统，其设计理念注重轻量化、最小化外部依赖、支持多平台，并提供广泛而灵活的硬件兼容性：\n",
    "\n",
    "- 纯 C/C++ 实现，无外部依赖\n",
    "\n",
    "- 支持广泛的硬件：\n",
    "\n",
    "  - x86_64 CPU 支持 AVX、AVX2 与 AVX512\n",
    "\n",
    "  - 通过 Metal 与 Accelerate 支持 Apple Silicon（CPU 与 GPU）\n",
    "\n",
    "  - 支持 NVIDIA GPU（通过 CUDA）、AMD GPU（通过 hipBLAS）、Intel GPU（通过 SYCL）、昇腾 NPU（通过 CANN）以及摩尔线程 GPU（通过 MUSA）\n",
    "\n",
    "  - 提供 GPU 的 Vulkan 后端\n",
    "\n",
    "- 提供多种量化方案，以提升推理速度并降低内存占用\n",
    "\n",
    "- 支持 CPU+GPU 混合推理，可加速运行超过显存容量的模型\n",
    "\n",
    "它类似于使用 Python 框架 `torch`+`transformers` 或 `torch`+`vllm`，但以 C++ 实现。然而，两者之间存在重要区别：\n",
    "\n",
    "- Python 是一种解释型语言：代码由解释器逐行执行，你可以通过解释器或交互式终端直接运行代码片段或脚本。Python 对初学者友好，即使了解不深，也较容易修改源码。\n",
    "\n",
    "- C++ 是一种编译型语言：源代码需先经编译转换为机器码与可执行文件，语言层面的开销极小。llama.cpp 同样提供了示例程序的源代码，展示如何使用该库。但若你不熟悉 C++ 或 C，修改源码将较为困难。\n",
    "\n",
    "真正“使用” llama.cpp 意味着将其作为库集成到自己的程序中，类似 Ollama、LM Studio、GPT4ALL、llamafile 等项目的实现方式。但这并非本指南的目标或所能覆盖的内容。这里我们将主要介绍如何使用 `llama-server` 和 `llama-cli` 示例程序，帮助你了解 llama.cpp 对 Qwen2.5 模型的支持，以及 llama.cpp 生态系统的基本运作方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f6b87-9ea6-4aed-a503-e5eeb6c169b1",
   "metadata": {},
   "source": [
    "### 1.2 Qwen3-VL介绍\n",
    "#### [认识Qwen3-VL系列模型](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct) — 迄今为止 Qwen 系列中功能最强大的视觉语言模型。\n",
    "这一代产品在各个方面都进行了全面升级：更优秀的文本理解和生成能力、更深入的视觉感知和推理能力、扩展的上下文长度、增强的空间和视频动态理解能力，以及更强的代理交互能力。\\\n",
    "提供从边缘到云端可扩展的 Dense 和 MoE 架构，并提供 Instruct 和增强推理的 Thinking 版本，以实现灵活、按需部署。\\\n",
    "主要增强功能：\n",
    "- 视觉代理：操作 PC/移动 GUI — 识别元素、理解功能、调用工具、完成任务。\n",
    "- 视觉编码增强：从图像/视频生成 Draw.io/HTML/CSS/JS。\n",
    "- 高级空间感知：判断物体位置、视角和遮挡；提供更强的 2D 基础，并支持 3D 基础，用于空间推理和具身 AI。\n",
    "- 长上下文和视频理解：原生 256K 上下文，可扩展至 1M；处理书籍和数小时的视频，具有完整的回忆和秒级索引。\n",
    "- 增强的多模态推理：在 STEM/数学方面表现出色 — 因果分析和基于逻辑、证据的答案。\n",
    "- 升级的视觉识别：更广泛、更高品质的预训练能够“识别一切”——名人、动漫、产品、地标、动植物等。\n",
    "- 扩展的 OCR：支持 32 种语言（从 19 种增加）；在低光、模糊和倾斜情况下表现稳健；更好地处理罕见/古代字符和术语；改进了长文档结构解析。\n",
    "- 与纯 LLM 相当的文本理解：无缝的文本-视觉融合，实现无损、统一的理解\n",
    "\n",
    "### 1.3 GGUF介绍\n",
    "GGUF是一种文件格式，用于存储运行模型所需的信息，包括但不限于模型权重、模型超参数、默认生成配置和tokenzier。\\\n",
    "在[Qwen3-VL-2B-Instruct-GGUF](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct-GGUF/summary)模型仓库中，仓库提供了 Qwen3-VL-2B-Instruct 的 GGUF 格式权重，分为两个组件：\n",
    "- 语言模型 (LLM)：FP16, Q8_0, Q4_K_M\n",
    "- 视觉编码器 (mmproj)：FP16, Q8_0\n",
    "这些文件与 llama.cpp、Ollama 和其他基于 GGUF 的工具兼容，支持在 CPU、NVIDIA GPU (CUDA)、Apple Silicon (Metal)、Intel GPUs (SYCL) 等上进行推理。您可以根据您的硬件和性能需求混合使用语言和视觉组件的精度级别，甚至可以从 FP16 权重开始进行自定义量化。\n",
    "\n",
    "### 1.4 获取“AI大脑”：模型文件\n",
    "Qwen3-VL需要两个核心文件，分别负责“语言”和“视觉”：\n",
    "- 主模型文件 (*.gguf)：语言理解与生成的核心。\n",
    "- 视觉投影文件 (mmproj-*.gguf)：将图像信息转换为模型可理解的“语言”。\n",
    "- 模型文件：`GGUF格式`极大简化了大语言模型文件的管理，可通过单模型文件完成推理。而且借助llama.cpp提供的丰富量化能力，一个模型repo下的不同GGUF文件，通常对应的是不同量化精度与量化方法。本教程默认选用的是Q4_K_M版本，在推理精度以及推理速度，资源消耗之间做一个较好的均衡。如果有特殊的需求，也可以选择更高的精度--例如FP16版本。\n",
    "- tips：本文用到的是`Qwen/Qwen3-VL-2B-Instruct-GGUF`模型，[点击即可跳转查看完整的模型文件](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct-GGUF/files)。\n",
    "  \n",
    "我们可以从[ModelScope社区下载模型文件](https://modelscope.cn/docs/models/download)(以下两种方式任选一种即可)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bc411-1fec-43f6-914e-d7d999606015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简洁版 ---- 一行命令下载所需文件（使用命令行工具下载）\n",
    "!modelscope download --model 'Qwen/Qwen3-VL-2B-Instruct-GGUF' Qwen3VL-2B-Instruct-Q4_K_M.gguf mmproj-Qwen3VL-2B-Instruct-F16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1bcdf-8d6b-4207-9583-764f11880d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装下载工具 使用 ModelScope SDK 下载\n",
    "!pip install modelscope --upgrade\n",
    "\n",
    "import os\n",
    "from modelscope import model_file_download\n",
    "\n",
    "# 指定模型 (以2B参数、平衡量化版为例)\n",
    "model_id = 'Qwen/Qwen3-VL-2B-Instruct-GGUF'\n",
    "gguf_file = 'Qwen3VL-2B-Instruct-Q4_K_M.gguf'\n",
    "mmproj_file = 'mmproj-Qwen3VL-2B-Instruct-F16.gguf'\n",
    "\n",
    "print(\"⏳ 开始下载模型文件 (约1-2GB)，请稍候...\")\n",
    "model_path = model_file_download(model_id, gguf_file)\n",
    "mmproj_path = model_file_download(model_id, mmproj_file)\n",
    "\n",
    "print(f\"\\n🎉 下载完成！\")\n",
    "print(f\"   语言模型: {model_path}\")\n",
    "print(f\"   视觉模型: {mmproj_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373d5f1",
   "metadata": {},
   "source": [
    "- 注意：这里输出的两个文件路径后面会用到，后续也可以尝试已经获取的`model_path`和`mmproj_path`变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a7857-aa63-4d9d-93cb-bdfad05074da",
   "metadata": {},
   "source": [
    "## 🔧 第二章：编译llama.cpp引擎\n",
    "llama.cpp是一个高效运行大模型的C++工具库，我们需要将它编译成可执行文件。\n",
    "\n",
    "### 2.1 获取源码\n",
    "在Notebook新单元格中执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dfd5c-8b8c-4c98-b89c-75eb1943aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "print(\"源代码克隆完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec22025-527d-4bab-bf1a-cddcb65b35cf",
   "metadata": {},
   "source": [
    "### 2.2 一键编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d9621-5089-4ca4-8aec-25d825122cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随时使用pwd命令查看你的当前工作目录\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4034b-9adc-4738-a7e1-0997a69012e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并进入构建目录\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbe48e-978b-4119-a18c-e4c0ed4bb033",
   "metadata": {},
   "source": [
    "## 🚀 第三章：启动你的AI助手\n",
    "这是最关键的一步，我们将启动模型服务。\n",
    "\n",
    "### 3.1 理解启动“密语”\n",
    "启动多模态服务需要两个关键指令，缺一不可：\n",
    "\n",
    "- 主模型路径 (-m)：指向下载的.gguf主模型文件;\n",
    "- 视觉投影路径 (--mmproj)：指向下载的mmproj文件。\n",
    "- tips:因为我们这里使用的是`-m model.gguf`的方式来使用llama.cpp的，所以需要使用`--mmproj file.gguf`额外指定**视觉投影文件**\n",
    "- 具体的&额外的使用方法可以参考[llama.cpp官方文档](https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md)\n",
    "\n",
    "### 3.2 在Notebook中优雅地启动\n",
    "llama-server 是一个简单的 HTTP 服务器，包含一组 LLM REST API 和一个简单的 Web 前端，用于通过 llama.cpp 与大型语言模型交互。\\\n",
    "默认情况下，服务器将在`http://localhost:8080`监听。所以你也可以打开网页，通过llama.cpp与qwen3-vl模型进行交互。\\\n",
    "为了更好管理，我们使用一个封装函数启动llama-server："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21aeacf-17ab-4bce-82ad-cc87d47dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, requests\n",
    "\n",
    "def start_ai_server():\n",
    "    \"\"\"一键启动Qwen3-VL多模态AI服务器\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"🚀 正在启动你的Qwen3-VL AI助手服务器\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. 如果遇到路径错误，请修改为你的实际文件路径！查看在1.4中的输出结果，或者尝试已经获取的`model_path`和`mmproj_path`变量。\n",
    "    # server可以使用相对路径，例如`build/bin/llama-server`。\n",
    "    # 模型文件我们使用已经获取的`model_path`和`mmproj_path`变量。\n",
    "    # 后续的路径同理。\n",
    "    paths = {\n",
    "        'server': './build/bin/llama-server',\n",
    "        'model': model_path,\n",
    "        'mmproj': mmproj_path\n",
    "    }\n",
    "    \n",
    "    # 2. 组装启动命令\n",
    "    cmd = [\n",
    "        str(paths['server']), '-m', str(paths['model']),\n",
    "        '--mmproj', str(paths['mmproj']),  # 多模态的关键！\n",
    "        '-c', '4096',  # 上下文长度，可根据需要调小以节省内存\n",
    "        '--host', '0.0.0.0', '--port', '8080'\n",
    "    ]\n",
    "    \n",
    "    # 3. 启动！\n",
    "    print(\"\\n⏳ 启动服务器进程中...（首次加载模型需要1-3分钟，请稍候）\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    # 等待并检查\n",
    "    time.sleep(5)  # 短暂等待进程启动\n",
    "    print(\"\\n⏳ 正在等待服务器就绪...（预计需要1-3分钟）\")\n",
    "    max_wait = 180  # 最长等待3分钟\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait:\n",
    "        try:\n",
    "            resp = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "            if resp.status_code == 200:\n",
    "                print(f\"\\n🎊 服务器启动成功！\")\n",
    "                print(f\"   📍 本地访问: http://localhost:8080\")\n",
    "                print(f\"   💬 API已就绪: http://localhost:8080/v1/chat/completions\")\n",
    "                return process\n",
    "        except requests.exceptions.RequestException:\n",
    "            # 服务器尚未就绪，继续轮询\n",
    "            pass\n",
    "        time.sleep(10)  # 等待10秒后重试\n",
    "    \n",
    "    print(\"⏳ 服务器启动超时或仍在加载。请检查子进程输出。\")\n",
    "    print(\"   完成后可手动在浏览器访问 http://localhost:8080 查看\")\n",
    "    return process\n",
    "\n",
    "# 执行启动\n",
    "my_ai_server = start_ai_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a0034-6e89-40ec-8414-0412439b7c0a",
   "metadata": {},
   "source": [
    "### 3.3 同样的，我们可以将模型下载和启动服务合并为一步来执行\n",
    "需要注意的是，这种方式我们还是需要指定一下投影文件，使用 `--mmproj` 参数，如果不指定投影文件的话，就相当于[禁用了多模态](https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md)，只能对话聊天，不能发送图片。\\\n",
    "并且，使用`-hf`的方式，下载Qwen/Qwen3-VL-2B-Instruct-GGUF模型，只会下载Qwen3VL-2B-Instruct-Q4_K_M.gguf版本的模型文件，所以还需要手动下载投影文件（可以参考1.4中的下载方式）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89f4a0-8246-4e9b-bd82-184b92d54c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, requests\n",
    "from pathlib import Path\n",
    "\n",
    "def start_ai_server():\n",
    "    \"\"\"一键启动Qwen3-VL多模态AI服务器\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"🚀 正在启动你的Qwen3-VL AI助手服务器\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. 设置环境变量，从ModelScope下载模型\n",
    "    env = os.environ.copy()\n",
    "    env['MODEL_ENDPOINT'] = 'https://www.modelscope.cn/'\n",
    "    \n",
    "    # 1. 如果遇到路径错误，请修改为你的实际server和mmproj文件路径！\n",
    "    # 模型文件我们使用已经获取的`mmproj_path`变量。\n",
    "    path_server = './build/bin/llama-server'\n",
    "    path_mmproj = mmproj_path\n",
    "    \n",
    "    # 3. 组装启动命令\n",
    "    cmd = [\n",
    "        path_server,\n",
    "        '-hf', 'Qwen/Qwen3-VL-2B-Instruct-GGUF',\n",
    "        '--mmproj', path_mmproj,\n",
    "        '--jinja',\n",
    "        '-ngl', '99',\n",
    "        '-fa', 'auto',\n",
    "        '-sm', 'row',\n",
    "        '--temp', '0.6',\n",
    "        '--top-k', '20',\n",
    "        '--top-p', '0.95',\n",
    "        '--min-p', '0',\n",
    "        '-c', '4096',\n",
    "        '-n', '32768',\n",
    "        '--no-context-shift',\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # 4. 启动！\n",
    "    print(\"\\n⏳ 启动服务器进程中...（首次加载模型需要1-3分钟，请稍候）\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    # 等待并检查\n",
    "    time.sleep(5)  # 短暂等待进程启动\n",
    "    print(\"\\n⏳ 正在等待服务器就绪...（预计需要1-3分钟）\")\n",
    "    max_wait = 180  # 最长等待3分钟\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait:\n",
    "        try:\n",
    "            resp = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "            if resp.status_code == 200:\n",
    "                print(f\"\\n🎊 服务器启动成功！\")\n",
    "                print(f\"   📍 本地访问: http://localhost:8080\")\n",
    "                print(f\"   💬 API已就绪: http://localhost:8080/v1/chat/completions\")\n",
    "                return process\n",
    "        except requests.exceptions.RequestException:\n",
    "            # 服务器尚未就绪，继续轮询\n",
    "            pass\n",
    "        time.sleep(10)  # 等待10秒后重试\n",
    "    \n",
    "    print(\"⏳ 服务器启动超时或仍在加载。请检查子进程输出。\")\n",
    "    print(\"   完成后可手动在浏览器访问 http://localhost:8080 查看\")\n",
    "    return process\n",
    "\n",
    "# 执行启动\n",
    "my_ai_server = start_ai_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fedb2-ecc1-48d6-aa2f-2947f076925b",
   "metadata": {},
   "source": [
    "当然，你也可以通过我们开头提到的，`极速预览`中的第三步，直接在**终端Terminal中**使用命令行的方式来启动服务器，效果是一样的。\n",
    "```python\n",
    "  ./build/bin/llama-server \\\n",
    "  -m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "  --mmproj /your/path/to/modelscope/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "  -c 4096 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98adbe7c-7ca8-4fb8-a89d-941b0d50dc18",
   "metadata": {},
   "source": [
    "## 🧪 第四站：对话测试 & 炫酷应用\n",
    "服务器跑起来后，让我们试试效果。\n",
    "\n",
    "### 4.1 基础测试：让AI描述你的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9488a8b-f12e-4218-af05-aa9844d27844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json\n",
    "\n",
    "def ask_ai_about_image(image_path, question=\"描述这张图片\"):\n",
    "    \"\"\"发送图片和问题给AI助手\"\"\"\n",
    "    # 将图片转为Base64\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    # 构建请求\n",
    "    url = \"http://localhost:8080/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "            ]\n",
    "        }],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    \n",
    "    print(f\"📤 询问AI: '{question}'\")\n",
    "    resp = requests.post(url, json=data, headers=headers, timeout=60)\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        reply = resp.json()['choices'][0]['message']['content']\n",
    "        print(f\"\\n🤖 AI助手回复:\\n{'-'*40}\\n{reply}\\n{'-'*40}\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(f\"请求出错: {resp.status_code}\")\n",
    "        return None\n",
    "\n",
    "# 这里我们下载一张边牧的图片，可以不下载(注释掉)替换为你的图片路径，也可以修改问题\n",
    "!wget -q --show-progress https://modelscope.oss-cn-beijing.aliyuncs.com/Dog.png -O dog.png\n",
    "ask_ai_about_image(\"dog.png\", \"图片里有什么？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86b093-ecf2-4942-bc23-e17930f2e538",
   "metadata": {},
   "source": [
    "### 4.2 使用OpenAI格式进行对话请求\n",
    "其中，api_key和model字段此处没有作用，但是要填上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46c94f-46f4-4934-9ed1-b9fa1807af7a",
   "metadata": {},
   "source": [
    "#### 初始化客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d043c92-dd66-4f71-a0e8-6bf6ea30b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:8080/v1',\n",
    "    api_key='not-needed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ec950-818e-4dd1-a9ed-550d935ad9e7",
   "metadata": {},
   "source": [
    "#### 纯文本聊天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f99341-2d24-48db-983a-19547af4a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单提问\n",
    "response = client.chat.completions.create(\n",
    "    model='qwen3-vl',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': '你好，请介绍一下你自己'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(f\"Q: 你好，请介绍一下你自己\")\n",
    "print(f\"A: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8864dc-2185-44c4-947a-673338b5252e",
   "metadata": {},
   "source": [
    "#### 多模态--带图片的聊天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432457c-dd12-4e7f-aa10-475d5899cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# 读取图片并转换为Base64\n",
    "image_path = \"dog.png\"  # 替换为你的图片路径，或使用之前下载的 dog.png\n",
    "\n",
    "with open(image_path, \"rb\") as f:\n",
    "    img_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "# 发送流式请求\n",
    "response = client.chat.completions.create(\n",
    "    model='qwen3-vl',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': '详细描述这张图片的内容'},\n",
    "            {\n",
    "                'type': 'image_url',\n",
    "                'image_url': {\n",
    "                    'url': f'data:image/jpeg;base64,{img_b64}'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }],\n",
    "    stream=True,  # 启用流式响应\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"正在分析图片，流式回复开始：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end='', flush=True)  # 逐字显示\n",
    "        full_response += content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf7067-a5e9-472d-a70f-3f6802000e07",
   "metadata": {},
   "source": [
    "### 4.3 使用curl进行请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398970b-0f03-45e6-a47c-a2f1c614e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8080/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"qwen3-vl\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"你好，请介绍一下你自己\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300,\n",
    "  \"temperature\": 0.6\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548d945-c7ca-48fd-9146-b649d6fc2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 这里可以选择将图片编码，或者选择在下面的请求中，将图片地址传入'url'参数，我们下载一张奥黛丽·赫本的图片为例\n",
    "wget -q --show-progress https://modelscope.oss-cn-beijing.aliyuncs.com/demo/images/audrey_hepburn.jpg -O audrey_hepburn.jpg\n",
    "IMAGE_B64=$(base64 -i audrey_hepburn.jpg | tr -d '\\n' | sed 's/\"/\\\\\"/g; s/\\\\/\\\\\\\\/g')\n",
    "\n",
    "curl -X POST http://localhost:8080/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"qwen3-vl\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"图片里有什么？\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"data:image/jpg;base64,'\"${IMAGE_B64}\"'\" \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300,\n",
    "  \"temperature\": 0.6\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4a9e5-a391-42b9-b4f2-58acacaf73f7",
   "metadata": {},
   "source": [
    "#### 健康检查：\n",
    "随时运行以下代码，检查助手是否“健康在线”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdc313-7fe8-4771-a7d9-72931d111b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:8080/health\", timeout=3)\n",
    "    print(f\"✅ AI助手状态健康 (HTTP {resp.status_code})\")\n",
    "except:\n",
    "    print(\"❌ AI助手服务未响应，请检查是否已启动。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ff41b-3a55-45f1-b1e9-2b7961514ff8",
   "metadata": {},
   "source": [
    "## 📚 总结与展望\n",
    "恭喜你！🎉 至此，你已经成功在本地部署了一个功能强大的多模态AI助手。我们来回顾一下核心要点：\n",
    "\n",
    "- **核心认知**：部署Qwen3-VL需要 “两个文件”（主模型+视觉投影)。\n",
    "\n",
    "- **核心步骤**：准备模型 -> 编译引擎 -> 启动服务 -> 对话测试。\n",
    "\n",
    "- **关键技巧**：在Notebook中使用subprocess管理进程，使用绝对路径，并对首次加载保持耐心。\n",
    "\n",
    "资源导航：\n",
    "\n",
    "- **[llama.cpp官方GitHub](https://github.com/ggml-org/llama.cpp)**：关注更新，获取文档。\n",
    "\n",
    "- **[ModelScope模型社区](https://modelscope.cn/models)**：海量开源模型免费下载。\n",
    "\n",
    "- **[Qwen官方文档](https://qwen.readthedocs.io/zh-cn/latest/run_locally/llama.cpp.html)**：深入了解模型特性。\n",
    "\n",
    "希望本教程能帮助你顺利开启本地AI探索之旅。祝你玩得开心！✨\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

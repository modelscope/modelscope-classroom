{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf620d8-edad-4b3d-964d-35e5242d2062",
   "metadata": {},
   "source": [
    "# ğŸ¤– åœ¨Jupyter Notebookä¸­ç©è½¬Qwen3-VLè§†è§‰å¤§æ¨¡å‹ï¼šä»éƒ¨ç½²åˆ°å¯¹è¯å…¨æŒ‡å—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028dc89-54c6-456e-9789-2b53f346cda9",
   "metadata": {},
   "source": [
    "éšç€å¤§æ¨¡å‹çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç›¸ä¿¡å¤§å®¶å·²ç»ä¸å†æ»¡è¶³äºåªèƒ½ä¸åŠ©æ‰‹èŠå¤©çš„å•ä¸€åŠŸèƒ½ï¼Œå¯¹å¤§æ¨¡å‹å¤„ç†å›¾åƒçš„éœ€æ±‚ä¹Ÿæ—¥æ¸å˜å¤šã€‚æœ¬æ•™ç¨‹å°†æ‰‹æŠŠæ‰‹å¸¦ä½ ï¼Œåœ¨ç†Ÿæ‚‰çš„Jupyter Notebookç¯å¢ƒä¸­ï¼Œé€šè¿‡llama.cppè½»æ¾éƒ¨ç½²å¼ºå¤§çš„Qwen3-VLå¤šæ¨¡æ€æ¨¡å‹ã€‚å‘Šåˆ«å¤æ‚çš„å‘½ä»¤è¡Œå’Œäº‘æœåŠ¡ä¾èµ–ï¼Œåªéœ€ä¸€æ­¥æ­¥è·Ÿéšï¼Œå³å¯åœ¨æœ¬åœ°æ„å»ºä¸€ä¸ªæ”¯æŒå›¾åƒç†è§£ä¸æ™ºèƒ½å¯¹è¯çš„ç§äººåŠ©æ‰‹ã€‚\n",
    "æ ¸å¿ƒä¼˜åŠ¿ï¼š\n",
    "- **å®Œå…¨æœ¬åœ°åŒ–**ï¼šæ¨¡å‹åœ¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œï¼Œéšç§æ— å¿§ã€‚\n",
    "\n",
    "- **å¤šæ¨¡æ€èƒ½åŠ›**ï¼šä¸ä»…ä¼šèŠå¤©ï¼Œæ›´èƒ½ç†è§£ä½ å‘é€çš„å›¾ç‰‡å†…å®¹ã€‚\n",
    "\n",
    "- **Notebookå‹å¥½**ï¼šæ‰€æœ‰æ“ä½œå‡åœ¨Jupyterå•å…ƒæ ¼å†…å®Œæˆï¼Œäº¤äº’ç›´è§‚ã€‚\n",
    "\n",
    "- **å…è´¹å¼€æº**ï¼šä¾æ‰˜llama.cppå’ŒModelScopeå¼€æºç¤¾åŒºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa457f47-2cbe-4301-9a71-02892cc671d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ¯ æé€Ÿé¢„è§ˆï¼šæˆåŠŸéƒ¨ç½²â€œä¸‰æ­¥æ›²â€\n",
    "å¦‚æœä½ å–œæ¬¢ç›´å…¥ä¸»é¢˜ï¼Œä»¥ä¸‹æ˜¯å·²éªŒè¯çš„æˆåŠŸè·¯å¾„æ‘˜è¦ï¼šï¼ˆ---åœ¨ç»ˆç«¯ä¸­æ‰§è¡Œå‘½ä»¤---ï¼‰\n",
    "### ç¬¬1æ­¥ï¼šä»ModelScopeä¸‹è½½Qwen3æ¨¡å‹æ–‡ä»¶\n",
    "```python\n",
    "pip install modelscope --upgrade\n",
    "modelscope download --model 'Qwen/Qwen3-VL-2B-Instruct-GGUF' Qwen3VL-2B-Instruct-Q4_K_M.gguf mmproj-Qwen3VL-2B-Instruct-F16.gguf (ä¸‹è½½åˆ°é»˜è®¤cacheåœ°å€)\n",
    "```\n",
    "\n",
    "### ç¬¬2æ­¥ï¼šè·å–å¹¶ç¼–è¯‘æ¨¡å‹å¼•æ“ (llama.cpp)\n",
    "```python\n",
    "git clone https://github.com/ggml-org/llama.cpp\n",
    "cd llama.cpp\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)\n",
    "```\n",
    "\n",
    "### ç¬¬3æ­¥ï¼šå¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ï¼ˆæ›¿æ¢å®é™…è·¯å¾„ï¼‰\n",
    "```\n",
    "  ./build/bin/llama-server \\\n",
    "  -m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "  --mmproj /your/path/to/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "  -c 4096 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "  ```\n",
    "\n",
    "### ç¬¬4æ­¥ï¼šå¯åŠ¨æ¨¡å‹äº¤äº’ç•Œé¢ï¼ˆå¯é€‰ï¼‰\n",
    "```\n",
    "./build/bin/llama-cli \\\n",
    "-m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "--mmproj /your/path/to/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "--jinja \\\n",
    "--color auto\n",
    "```\n",
    "\n",
    "### æ³¨æ„----ç¬¬ä¸€æ­¥æ¨¡å‹ä¸‹è½½å’Œç¬¬ä¸‰æ­¥çš„å¯åŠ¨serverå¯ä»¥ç”¨ä¸€æ­¥æ¥æå®šï¼Œå³ï¼š\n",
    "ä½†æ˜¯è¦æ³¨æ„åŠ ä¸Š`--mmproj`å‚æ•°æ¥æŒ‡å®šæŠ•å½±æ–‡ä»¶ï¼Œå¯ç”¨å¤šæ¨¡æ€\n",
    "```\n",
    "export MODEL_ENDPOINT=https://www.modelscope.cn/\n",
    "./build/bin/llama-server -hf Qwen/Qwen3-VL-2B-Instruct-GGUF --jinja -ngl 99 -fa auto -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift\n",
    "```\n",
    "ç±»ä¼¼çš„ï¼Œç¬¬ä¸€æ­¥å’Œç¬¬å››æ­¥ä¹Ÿå¯ä»¥åˆå¹¶ä¸ºä¸€æ­¥ï¼š\n",
    "```\n",
    "export MODEL_ENDPOINT=https://www.modelscope.cn/\n",
    "./build/bin/llama-cli -hf Qwen/Qwen3-VL-2B-Instruct-GGUF --jinja --color auto -ngl 99 -fa auto -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift\n",
    "```\n",
    "ï¼ˆè¿™æ ·é€šè¿‡è®¾ç½®`MODEL_ENDPOINT`ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥è®©ç¨‹åºä»ModelScopeä¸‹è½½æ¨¡å‹ï¼Œä¸‹è½½å®Œæˆä¹‹åç›´æ¥å¯åŠ¨ï¼‰\\\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬å°†è¯¦ç»†å±•å¼€æ¯ä¸€æ­¥åœ¨Notebookä¸­çš„ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81652a89-086d-4422-8995-793256a58b2f",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ç¬¬ä¸€ç« ï¼šæˆ˜å‰å‡†å¤‡ï¼ˆç¯å¢ƒä¸æ¨¡å‹ï¼‰\n",
    "\n",
    "### 1.1 è®¤è¯†llama.cpp\n",
    "å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç®€è¦ä»‹ç»ä¸€ä¸‹ llama.cppï¼Œè¯´æ˜ä½ å¯ä»¥ä»ä¸­è·å¾—ä»€ä¹ˆï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬è¦å¼ºè°ƒâ€œä½¿ç”¨â€ llama.cppã€‚æœ¬è´¨ä¸Šï¼Œllama.cpp æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç”Ÿæ€ç³»ç»Ÿï¼Œå…¶è®¾è®¡ç†å¿µæ³¨é‡è½»é‡åŒ–ã€æœ€å°åŒ–å¤–éƒ¨ä¾èµ–ã€æ”¯æŒå¤šå¹³å°ï¼Œå¹¶æä¾›å¹¿æ³›è€Œçµæ´»çš„ç¡¬ä»¶å…¼å®¹æ€§ï¼š\n",
    "\n",
    "- çº¯ C/C++ å®ç°ï¼Œæ— å¤–éƒ¨ä¾èµ–\n",
    "\n",
    "- æ”¯æŒå¹¿æ³›çš„ç¡¬ä»¶ï¼š\n",
    "\n",
    "  - x86_64 CPU æ”¯æŒ AVXã€AVX2 ä¸ AVX512\n",
    "\n",
    "  - é€šè¿‡ Metal ä¸ Accelerate æ”¯æŒ Apple Siliconï¼ˆCPU ä¸ GPUï¼‰\n",
    "\n",
    "  - æ”¯æŒ NVIDIA GPUï¼ˆé€šè¿‡ CUDAï¼‰ã€AMD GPUï¼ˆé€šè¿‡ hipBLASï¼‰ã€Intel GPUï¼ˆé€šè¿‡ SYCLï¼‰ã€æ˜‡è…¾ NPUï¼ˆé€šè¿‡ CANNï¼‰ä»¥åŠæ‘©å°”çº¿ç¨‹ GPUï¼ˆé€šè¿‡ MUSAï¼‰\n",
    "\n",
    "  - æä¾› GPU çš„ Vulkan åç«¯\n",
    "\n",
    "- æä¾›å¤šç§é‡åŒ–æ–¹æ¡ˆï¼Œä»¥æå‡æ¨ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜å ç”¨\n",
    "\n",
    "- æ”¯æŒ CPU+GPU æ··åˆæ¨ç†ï¼Œå¯åŠ é€Ÿè¿è¡Œè¶…è¿‡æ˜¾å­˜å®¹é‡çš„æ¨¡å‹\n",
    "\n",
    "å®ƒç±»ä¼¼äºä½¿ç”¨ Python æ¡†æ¶ `torch`+`transformers` æˆ– `torch`+`vllm`ï¼Œä½†ä»¥ C++ å®ç°ã€‚ç„¶è€Œï¼Œä¸¤è€…ä¹‹é—´å­˜åœ¨é‡è¦åŒºåˆ«ï¼š\n",
    "\n",
    "- Python æ˜¯ä¸€ç§è§£é‡Šå‹è¯­è¨€ï¼šä»£ç ç”±è§£é‡Šå™¨é€è¡Œæ‰§è¡Œï¼Œä½ å¯ä»¥é€šè¿‡è§£é‡Šå™¨æˆ–äº¤äº’å¼ç»ˆç«¯ç›´æ¥è¿è¡Œä»£ç ç‰‡æ®µæˆ–è„šæœ¬ã€‚Python å¯¹åˆå­¦è€…å‹å¥½ï¼Œå³ä½¿äº†è§£ä¸æ·±ï¼Œä¹Ÿè¾ƒå®¹æ˜“ä¿®æ”¹æºç ã€‚\n",
    "\n",
    "- C++ æ˜¯ä¸€ç§ç¼–è¯‘å‹è¯­è¨€ï¼šæºä»£ç éœ€å…ˆç»ç¼–è¯‘è½¬æ¢ä¸ºæœºå™¨ç ä¸å¯æ‰§è¡Œæ–‡ä»¶ï¼Œè¯­è¨€å±‚é¢çš„å¼€é”€æå°ã€‚llama.cpp åŒæ ·æä¾›äº†ç¤ºä¾‹ç¨‹åºçš„æºä»£ç ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥åº“ã€‚ä½†è‹¥ä½ ä¸ç†Ÿæ‚‰ C++ æˆ– Cï¼Œä¿®æ”¹æºç å°†è¾ƒä¸ºå›°éš¾ã€‚\n",
    "\n",
    "çœŸæ­£â€œä½¿ç”¨â€ llama.cpp æ„å‘³ç€å°†å…¶ä½œä¸ºåº“é›†æˆåˆ°è‡ªå·±çš„ç¨‹åºä¸­ï¼Œç±»ä¼¼ Ollamaã€LM Studioã€GPT4ALLã€llamafile ç­‰é¡¹ç›®çš„å®ç°æ–¹å¼ã€‚ä½†è¿™å¹¶éæœ¬æŒ‡å—çš„ç›®æ ‡æˆ–æ‰€èƒ½è¦†ç›–çš„å†…å®¹ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä¸»è¦ä»‹ç»å¦‚ä½•ä½¿ç”¨ `llama-server` å’Œ `llama-cli` ç¤ºä¾‹ç¨‹åºï¼Œå¸®åŠ©ä½ äº†è§£ llama.cpp å¯¹ Qwen2.5 æ¨¡å‹çš„æ”¯æŒï¼Œä»¥åŠ llama.cpp ç”Ÿæ€ç³»ç»Ÿçš„åŸºæœ¬è¿ä½œæ–¹å¼ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f6b87-9ea6-4aed-a503-e5eeb6c169b1",
   "metadata": {},
   "source": [
    "### 1.2 Qwen3-VLä»‹ç»\n",
    "#### [è®¤è¯†Qwen3-VLç³»åˆ—æ¨¡å‹](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct) â€” è¿„ä»Šä¸ºæ­¢ Qwen ç³»åˆ—ä¸­åŠŸèƒ½æœ€å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚\n",
    "è¿™ä¸€ä»£äº§å“åœ¨å„ä¸ªæ–¹é¢éƒ½è¿›è¡Œäº†å…¨é¢å‡çº§ï¼šæ›´ä¼˜ç§€çš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€æ›´æ·±å…¥çš„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€æ‰©å±•çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€å¢å¼ºçš„ç©ºé—´å’Œè§†é¢‘åŠ¨æ€ç†è§£èƒ½åŠ›ï¼Œä»¥åŠæ›´å¼ºçš„ä»£ç†äº¤äº’èƒ½åŠ›ã€‚\\\n",
    "æä¾›ä»è¾¹ç¼˜åˆ°äº‘ç«¯å¯æ‰©å±•çš„ Dense å’Œ MoE æ¶æ„ï¼Œå¹¶æä¾› Instruct å’Œå¢å¼ºæ¨ç†çš„ Thinking ç‰ˆæœ¬ï¼Œä»¥å®ç°çµæ´»ã€æŒ‰éœ€éƒ¨ç½²ã€‚\\\n",
    "ä¸»è¦å¢å¼ºåŠŸèƒ½ï¼š\n",
    "- è§†è§‰ä»£ç†ï¼šæ“ä½œ PC/ç§»åŠ¨ GUI â€” è¯†åˆ«å…ƒç´ ã€ç†è§£åŠŸèƒ½ã€è°ƒç”¨å·¥å…·ã€å®Œæˆä»»åŠ¡ã€‚\n",
    "- è§†è§‰ç¼–ç å¢å¼ºï¼šä»å›¾åƒ/è§†é¢‘ç”Ÿæˆ Draw.io/HTML/CSS/JSã€‚\n",
    "- é«˜çº§ç©ºé—´æ„ŸçŸ¥ï¼šåˆ¤æ–­ç‰©ä½“ä½ç½®ã€è§†è§’å’Œé®æŒ¡ï¼›æä¾›æ›´å¼ºçš„ 2D åŸºç¡€ï¼Œå¹¶æ”¯æŒ 3D åŸºç¡€ï¼Œç”¨äºç©ºé—´æ¨ç†å’Œå…·èº« AIã€‚\n",
    "- é•¿ä¸Šä¸‹æ–‡å’Œè§†é¢‘ç†è§£ï¼šåŸç”Ÿ 256K ä¸Šä¸‹æ–‡ï¼Œå¯æ‰©å±•è‡³ 1Mï¼›å¤„ç†ä¹¦ç±å’Œæ•°å°æ—¶çš„è§†é¢‘ï¼Œå…·æœ‰å®Œæ•´çš„å›å¿†å’Œç§’çº§ç´¢å¼•ã€‚\n",
    "- å¢å¼ºçš„å¤šæ¨¡æ€æ¨ç†ï¼šåœ¨ STEM/æ•°å­¦æ–¹é¢è¡¨ç°å‡ºè‰² â€” å› æœåˆ†æå’ŒåŸºäºé€»è¾‘ã€è¯æ®çš„ç­”æ¡ˆã€‚\n",
    "- å‡çº§çš„è§†è§‰è¯†åˆ«ï¼šæ›´å¹¿æ³›ã€æ›´é«˜å“è´¨çš„é¢„è®­ç»ƒèƒ½å¤Ÿâ€œè¯†åˆ«ä¸€åˆ‡â€â€”â€”åäººã€åŠ¨æ¼«ã€äº§å“ã€åœ°æ ‡ã€åŠ¨æ¤ç‰©ç­‰ã€‚\n",
    "- æ‰©å±•çš„ OCRï¼šæ”¯æŒ 32 ç§è¯­è¨€ï¼ˆä» 19 ç§å¢åŠ ï¼‰ï¼›åœ¨ä½å…‰ã€æ¨¡ç³Šå’Œå€¾æ–œæƒ…å†µä¸‹è¡¨ç°ç¨³å¥ï¼›æ›´å¥½åœ°å¤„ç†ç½•è§/å¤ä»£å­—ç¬¦å’Œæœ¯è¯­ï¼›æ”¹è¿›äº†é•¿æ–‡æ¡£ç»“æ„è§£æã€‚\n",
    "- ä¸çº¯ LLM ç›¸å½“çš„æ–‡æœ¬ç†è§£ï¼šæ— ç¼çš„æ–‡æœ¬-è§†è§‰èåˆï¼Œå®ç°æ— æŸã€ç»Ÿä¸€çš„ç†è§£\n",
    "\n",
    "### 1.3 GGUFä»‹ç»\n",
    "GGUFæ˜¯ä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºå­˜å‚¨è¿è¡Œæ¨¡å‹æ‰€éœ€çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ¨¡å‹æƒé‡ã€æ¨¡å‹è¶…å‚æ•°ã€é»˜è®¤ç”Ÿæˆé…ç½®å’Œtokenzierã€‚\\\n",
    "åœ¨[Qwen3-VL-2B-Instruct-GGUF](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct-GGUF/summary)æ¨¡å‹ä»“åº“ä¸­ï¼Œä»“åº“æä¾›äº† Qwen3-VL-2B-Instruct çš„ GGUF æ ¼å¼æƒé‡ï¼Œåˆ†ä¸ºä¸¤ä¸ªç»„ä»¶ï¼š\n",
    "- è¯­è¨€æ¨¡å‹ (LLM)ï¼šFP16, Q8_0, Q4_K_M\n",
    "- è§†è§‰ç¼–ç å™¨ (mmproj)ï¼šFP16, Q8_0\n",
    "è¿™äº›æ–‡ä»¶ä¸ llama.cppã€Ollama å’Œå…¶ä»–åŸºäº GGUF çš„å·¥å…·å…¼å®¹ï¼Œæ”¯æŒåœ¨ CPUã€NVIDIA GPU (CUDA)ã€Apple Silicon (Metal)ã€Intel GPUs (SYCL) ç­‰ä¸Šè¿›è¡Œæ¨ç†ã€‚æ‚¨å¯ä»¥æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œæ€§èƒ½éœ€æ±‚æ··åˆä½¿ç”¨è¯­è¨€å’Œè§†è§‰ç»„ä»¶çš„ç²¾åº¦çº§åˆ«ï¼Œç”šè‡³å¯ä»¥ä» FP16 æƒé‡å¼€å§‹è¿›è¡Œè‡ªå®šä¹‰é‡åŒ–ã€‚\n",
    "\n",
    "### 1.4 è·å–â€œAIå¤§è„‘â€ï¼šæ¨¡å‹æ–‡ä»¶\n",
    "Qwen3-VLéœ€è¦ä¸¤ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£â€œè¯­è¨€â€å’Œâ€œè§†è§‰â€ï¼š\n",
    "- ä¸»æ¨¡å‹æ–‡ä»¶ (*.gguf)ï¼šè¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„æ ¸å¿ƒã€‚\n",
    "- è§†è§‰æŠ•å½±æ–‡ä»¶ (mmproj-*.gguf)ï¼šå°†å›¾åƒä¿¡æ¯è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„â€œè¯­è¨€â€ã€‚\n",
    "- æ¨¡å‹æ–‡ä»¶ï¼š`GGUFæ ¼å¼`æå¤§ç®€åŒ–äº†å¤§è¯­è¨€æ¨¡å‹æ–‡ä»¶çš„ç®¡ç†ï¼Œå¯é€šè¿‡å•æ¨¡å‹æ–‡ä»¶å®Œæˆæ¨ç†ã€‚è€Œä¸”å€ŸåŠ©llama.cppæä¾›çš„ä¸°å¯Œé‡åŒ–èƒ½åŠ›ï¼Œä¸€ä¸ªæ¨¡å‹repoä¸‹çš„ä¸åŒGGUFæ–‡ä»¶ï¼Œé€šå¸¸å¯¹åº”çš„æ˜¯ä¸åŒé‡åŒ–ç²¾åº¦ä¸é‡åŒ–æ–¹æ³•ã€‚æœ¬æ•™ç¨‹é»˜è®¤é€‰ç”¨çš„æ˜¯Q4_K_Mç‰ˆæœ¬ï¼Œåœ¨æ¨ç†ç²¾åº¦ä»¥åŠæ¨ç†é€Ÿåº¦ï¼Œèµ„æºæ¶ˆè€—ä¹‹é—´åšä¸€ä¸ªè¾ƒå¥½çš„å‡è¡¡ã€‚å¦‚æœæœ‰ç‰¹æ®Šçš„éœ€æ±‚ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©æ›´é«˜çš„ç²¾åº¦--ä¾‹å¦‚FP16ç‰ˆæœ¬ã€‚\n",
    "- tipsï¼šæœ¬æ–‡ç”¨åˆ°çš„æ˜¯`Qwen/Qwen3-VL-2B-Instruct-GGUF`æ¨¡å‹ï¼Œ[ç‚¹å‡»å³å¯è·³è½¬æŸ¥çœ‹å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct-GGUF/files)ã€‚\n",
    "  \n",
    "æˆ‘ä»¬å¯ä»¥ä»[ModelScopeç¤¾åŒºä¸‹è½½æ¨¡å‹æ–‡ä»¶](https://modelscope.cn/docs/models/download)(ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä»»é€‰ä¸€ç§å³å¯)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bc411-1fec-43f6-914e-d7d999606015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€æ´ç‰ˆ ---- ä¸€è¡Œå‘½ä»¤ä¸‹è½½æ‰€éœ€æ–‡ä»¶ï¼ˆä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·ä¸‹è½½ï¼‰\n",
    "!modelscope download --model 'Qwen/Qwen3-VL-2B-Instruct-GGUF' Qwen3VL-2B-Instruct-Q4_K_M.gguf mmproj-Qwen3VL-2B-Instruct-F16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1bcdf-8d6b-4207-9583-764f11880d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¸‹è½½å·¥å…· ä½¿ç”¨ ModelScope SDK ä¸‹è½½\n",
    "!pip install modelscope --upgrade\n",
    "\n",
    "import os\n",
    "from modelscope import model_file_download\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹ (ä»¥2Bå‚æ•°ã€å¹³è¡¡é‡åŒ–ç‰ˆä¸ºä¾‹)\n",
    "model_id = 'Qwen/Qwen3-VL-2B-Instruct-GGUF'\n",
    "gguf_file = 'Qwen3VL-2B-Instruct-Q4_K_M.gguf'\n",
    "mmproj_file = 'mmproj-Qwen3VL-2B-Instruct-F16.gguf'\n",
    "\n",
    "print(\"â³ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (çº¦1-2GB)ï¼Œè¯·ç¨å€™...\")\n",
    "model_path = model_file_download(model_id, gguf_file)\n",
    "mmproj_path = model_file_download(model_id, mmproj_file)\n",
    "\n",
    "print(f\"\\nğŸ‰ ä¸‹è½½å®Œæˆï¼\")\n",
    "print(f\"   è¯­è¨€æ¨¡å‹: {model_path}\")\n",
    "print(f\"   è§†è§‰æ¨¡å‹: {mmproj_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373d5f1",
   "metadata": {},
   "source": [
    "- æ³¨æ„ï¼šè¿™é‡Œè¾“å‡ºçš„ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„åé¢ä¼šç”¨åˆ°ï¼Œåç»­ä¹Ÿå¯ä»¥å°è¯•å·²ç»è·å–çš„`model_path`å’Œ`mmproj_path`å˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a7857-aa63-4d9d-93cb-bdfad05074da",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç¬¬äºŒç« ï¼šç¼–è¯‘llama.cppå¼•æ“\n",
    "llama.cppæ˜¯ä¸€ä¸ªé«˜æ•ˆè¿è¡Œå¤§æ¨¡å‹çš„C++å·¥å…·åº“ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒç¼–è¯‘æˆå¯æ‰§è¡Œæ–‡ä»¶ã€‚\n",
    "\n",
    "### 2.1 è·å–æºç \n",
    "åœ¨Notebookæ–°å•å…ƒæ ¼ä¸­æ‰§è¡Œï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dfd5c-8b8c-4c98-b89c-75eb1943aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "print(\"æºä»£ç å…‹éš†å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec22025-527d-4bab-bf1a-cddcb65b35cf",
   "metadata": {},
   "source": [
    "### 2.2 ä¸€é”®ç¼–è¯‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d9621-5089-4ca4-8aec-25d825122cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæ—¶ä½¿ç”¨pwdå‘½ä»¤æŸ¥çœ‹ä½ çš„å½“å‰å·¥ä½œç›®å½•\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4034b-9adc-4738-a7e1-0997a69012e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¹¶è¿›å…¥æ„å»ºç›®å½•\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbe48e-978b-4119-a18c-e4c0ed4bb033",
   "metadata": {},
   "source": [
    "## ğŸš€ ç¬¬ä¸‰ç« ï¼šå¯åŠ¨ä½ çš„AIåŠ©æ‰‹\n",
    "è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å¯åŠ¨æ¨¡å‹æœåŠ¡ã€‚\n",
    "\n",
    "### 3.1 ç†è§£å¯åŠ¨â€œå¯†è¯­â€\n",
    "å¯åŠ¨å¤šæ¨¡æ€æœåŠ¡éœ€è¦ä¸¤ä¸ªå…³é”®æŒ‡ä»¤ï¼Œç¼ºä¸€ä¸å¯ï¼š\n",
    "\n",
    "- ä¸»æ¨¡å‹è·¯å¾„ (-m)ï¼šæŒ‡å‘ä¸‹è½½çš„.ggufä¸»æ¨¡å‹æ–‡ä»¶;\n",
    "- è§†è§‰æŠ•å½±è·¯å¾„ (--mmproj)ï¼šæŒ‡å‘ä¸‹è½½çš„mmprojæ–‡ä»¶ã€‚\n",
    "- tips:å› ä¸ºæˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯`-m model.gguf`çš„æ–¹å¼æ¥ä½¿ç”¨llama.cppçš„ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨`--mmproj file.gguf`é¢å¤–æŒ‡å®š**è§†è§‰æŠ•å½±æ–‡ä»¶**\n",
    "- å…·ä½“çš„&é¢å¤–çš„ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒ[llama.cppå®˜æ–¹æ–‡æ¡£](https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md)\n",
    "\n",
    "### 3.2 åœ¨Notebookä¸­ä¼˜é›…åœ°å¯åŠ¨\n",
    "llama-server æ˜¯ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼ŒåŒ…å«ä¸€ç»„ LLM REST API å’Œä¸€ä¸ªç®€å•çš„ Web å‰ç«¯ï¼Œç”¨äºé€šè¿‡ llama.cpp ä¸å¤§å‹è¯­è¨€æ¨¡å‹äº¤äº’ã€‚\\\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨å°†åœ¨`http://localhost:8080`ç›‘å¬ã€‚æ‰€ä»¥ä½ ä¹Ÿå¯ä»¥æ‰“å¼€ç½‘é¡µï¼Œé€šè¿‡llama.cppä¸qwen3-vlæ¨¡å‹è¿›è¡Œäº¤äº’ã€‚\\\n",
    "ä¸ºäº†æ›´å¥½ç®¡ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå°è£…å‡½æ•°å¯åŠ¨llama-serverï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21aeacf-17ab-4bce-82ad-cc87d47dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, requests\n",
    "\n",
    "def start_ai_server():\n",
    "    \"\"\"ä¸€é”®å¯åŠ¨Qwen3-VLå¤šæ¨¡æ€AIæœåŠ¡å™¨\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"ğŸš€ æ­£åœ¨å¯åŠ¨ä½ çš„Qwen3-VL AIåŠ©æ‰‹æœåŠ¡å™¨\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„ï¼æŸ¥çœ‹åœ¨1.4ä¸­çš„è¾“å‡ºç»“æœï¼Œæˆ–è€…å°è¯•å·²ç»è·å–çš„`model_path`å’Œ`mmproj_path`å˜é‡ã€‚\n",
    "    # serverå¯ä»¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¾‹å¦‚`build/bin/llama-server`ã€‚\n",
    "    # åç»­çš„è·¯å¾„åŒç†ã€‚\n",
    "    paths = {\n",
    "        'server': '/your/path/to/llama-server',\n",
    "        'model': '/your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf',\n",
    "        'mmproj': '/your/path/to/mmproj-Qwen3VL-2B-Instruct-F16.gguf'\n",
    "    }\n",
    "    \n",
    "    # 2. ç»„è£…å¯åŠ¨å‘½ä»¤\n",
    "    cmd = [\n",
    "        str(paths['server']), '-m', str(paths['model']),\n",
    "        '--mmproj', str(paths['mmproj']),  # å¤šæ¨¡æ€çš„å…³é”®ï¼\n",
    "        '-c', '4096',  # ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¯æ ¹æ®éœ€è¦è°ƒå°ä»¥èŠ‚çœå†…å­˜\n",
    "        '--host', '0.0.0.0', '--port', '8080'\n",
    "    ]\n",
    "    \n",
    "    # 3. å¯åŠ¨ï¼\n",
    "    print(\"\\nâ³ å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹ä¸­...ï¼ˆé¦–æ¬¡åŠ è½½æ¨¡å‹éœ€è¦1-3åˆ†é’Ÿï¼Œè¯·ç¨å€™ï¼‰\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    # ç­‰å¾…å¹¶æ£€æŸ¥\n",
    "    time.sleep(20)\n",
    "    try:\n",
    "        resp = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            print(f\"\\nğŸŠ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼\")\n",
    "            print(f\"   ğŸ“ æœ¬åœ°è®¿é—®: http://localhost:8080\")\n",
    "            print(f\"   ğŸ’¬ APIå·²å°±ç»ª: http://localhost:8080/v1/chat/completions\")\n",
    "            return process\n",
    "    except:\n",
    "        print(\"â³ æœåŠ¡å™¨æ­£åœ¨åŠªåŠ›åŠ è½½æ¨¡å‹ï¼Œè¯·å†ç­‰å¾…ä¸€åˆ†é’Ÿ...\")\n",
    "        print(\"   å®Œæˆåå¯æ‰‹åŠ¨åœ¨æµè§ˆå™¨è®¿é—® http://localhost:8080 æŸ¥çœ‹\")\n",
    "        return process\n",
    "\n",
    "# æ‰§è¡Œå¯åŠ¨\n",
    "my_ai_server = start_ai_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a0034-6e89-40ec-8414-0412439b7c0a",
   "metadata": {},
   "source": [
    "### 3.3 åŒæ ·çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¸‹è½½å’Œå¯åŠ¨æœåŠ¡åˆå¹¶ä¸ºä¸€æ­¥æ¥æ‰§è¡Œ\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹å¼æˆ‘ä»¬è¿˜æ˜¯éœ€è¦æŒ‡å®šä¸€ä¸‹æŠ•å½±æ–‡ä»¶ï¼Œä½¿ç”¨ `--mmproj` å‚æ•°ï¼Œå¦‚æœä¸æŒ‡å®šæŠ•å½±æ–‡ä»¶çš„è¯ï¼Œå°±ç›¸å½“äº[ç¦ç”¨äº†å¤šæ¨¡æ€](https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md)ï¼Œåªèƒ½å¯¹è¯èŠå¤©ï¼Œä¸èƒ½å‘é€å›¾ç‰‡ã€‚\\\n",
    "å¹¶ä¸”ï¼Œä½¿ç”¨`-hf`çš„æ–¹å¼ï¼Œä¸‹è½½Qwen/Qwen3-VL-2B-Instruct-GGUFæ¨¡å‹ï¼Œåªä¼šä¸‹è½½Qwen3VL-2B-Instruct-Q4_K_M.ggufç‰ˆæœ¬çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ‰€ä»¥è¿˜éœ€è¦æ‰‹åŠ¨ä¸‹è½½æŠ•å½±æ–‡ä»¶ï¼ˆå¯ä»¥å‚è€ƒ1.4ä¸­çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89f4a0-8246-4e9b-bd82-184b92d54c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, time, requests\n",
    "from pathlib import Path\n",
    "\n",
    "def start_ai_server():\n",
    "    \"\"\"ä¸€é”®å¯åŠ¨Qwen3-VLå¤šæ¨¡æ€AIæœåŠ¡å™¨\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"ğŸš€ æ­£åœ¨å¯åŠ¨ä½ çš„Qwen3-VL AIåŠ©æ‰‹æœåŠ¡å™¨\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä»ModelScopeä¸‹è½½æ¨¡å‹\n",
    "    env = os.environ.copy()\n",
    "    env['MODEL_ENDPOINT'] = 'https://www.modelscope.cn/'\n",
    "    \n",
    "    # 1. è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…serverå’Œmmprojæ–‡ä»¶è·¯å¾„ï¼\n",
    "    path_server = '/your/path/to/llama-server'\n",
    "    path_mmproj = '/your/path/to/mmproj.gguf'\n",
    "    \n",
    "    # 3. ç»„è£…å¯åŠ¨å‘½ä»¤\n",
    "    cmd = [\n",
    "        path_server,\n",
    "        '-hf', 'Qwen/Qwen3-VL-2B-Instruct-GGUF',\n",
    "        '--mmproj', path_mmproj,\n",
    "        '--jinja',\n",
    "        '-ngl', '99',\n",
    "        '-fa', 'auto',\n",
    "        '-sm', 'row',\n",
    "        '--temp', '0.6',\n",
    "        '--top-k', '20',\n",
    "        '--top-p', '0.95',\n",
    "        '--min-p', '0',\n",
    "        '-c', '4096',\n",
    "        '-n', '32768',\n",
    "        '--no-context-shift',\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # 4. å¯åŠ¨ï¼\n",
    "    print(\"\\nâ³ å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹ä¸­...ï¼ˆé¦–æ¬¡åŠ è½½æ¨¡å‹éœ€è¦1-3åˆ†é’Ÿï¼Œè¯·ç¨å€™ï¼‰\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    # ç­‰å¾…å¹¶æ£€æŸ¥\n",
    "    time.sleep(20)\n",
    "    try:\n",
    "        resp = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            print(f\"\\nğŸŠ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼\")\n",
    "            print(f\"   ğŸ“ æœ¬åœ°è®¿é—®: http://localhost:8080\")\n",
    "            print(f\"   ğŸ’¬ APIå·²å°±ç»ª: http://localhost:8080/v1/chat/completions\")\n",
    "            return process\n",
    "    except:\n",
    "        print(\"â³ æœåŠ¡å™¨æ­£åœ¨åŠªåŠ›åŠ è½½æ¨¡å‹ï¼Œè¯·å†ç­‰å¾…ä¸€åˆ†é’Ÿ...\")\n",
    "        print(\"   å®Œæˆåå¯æ‰‹åŠ¨åœ¨æµè§ˆå™¨è®¿é—® http://localhost:8080 æŸ¥çœ‹\")\n",
    "        return process\n",
    "\n",
    "# æ‰§è¡Œå¯åŠ¨\n",
    "my_ai_server = start_ai_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fedb2-ecc1-48d6-aa2f-2947f076925b",
   "metadata": {},
   "source": [
    "å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡æˆ‘ä»¬å¼€å¤´æåˆ°çš„ï¼Œ`æé€Ÿé¢„è§ˆ`ä¸­çš„ç¬¬ä¸‰æ­¥ï¼Œç›´æ¥åœ¨**ç»ˆç«¯Terminalä¸­**ä½¿ç”¨å‘½ä»¤è¡Œçš„æ–¹å¼æ¥å¯åŠ¨æœåŠ¡å™¨ï¼Œæ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚\n",
    "```python\n",
    "  ./build/bin/llama-server \\\n",
    "  -m /your/path/to/Qwen3VL-2B-Instruct-Q4_K_M.gguf \\\n",
    "  --mmproj /your/path/to/modelscope/mmproj-Qwen3VL-2B-Instruct-F16.gguf \\\n",
    "  -c 4096 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98adbe7c-7ca8-4fb8-a89d-941b0d50dc18",
   "metadata": {},
   "source": [
    "## ğŸ§ª ç¬¬å››ç«™ï¼šå¯¹è¯æµ‹è¯• & ç‚«é…·åº”ç”¨\n",
    "æœåŠ¡å™¨è·‘èµ·æ¥åï¼Œè®©æˆ‘ä»¬è¯•è¯•æ•ˆæœã€‚\n",
    "\n",
    "### 4.1 åŸºç¡€æµ‹è¯•ï¼šè®©AIæè¿°ä½ çš„å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9488a8b-f12e-4218-af05-aa9844d27844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json\n",
    "\n",
    "def ask_ai_about_image(image_path, question=\"æè¿°è¿™å¼ å›¾ç‰‡\"):\n",
    "    \"\"\"å‘é€å›¾ç‰‡å’Œé—®é¢˜ç»™AIåŠ©æ‰‹\"\"\"\n",
    "    # å°†å›¾ç‰‡è½¬ä¸ºBase64\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    # æ„å»ºè¯·æ±‚\n",
    "    url = \"http://0.0.0.0:8080/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "            ]\n",
    "        }],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“¤ è¯¢é—®AI: '{question}'\")\n",
    "    resp = requests.post(url, json=data, headers=headers, timeout=60)\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        reply = resp.json()['choices'][0]['message']['content']\n",
    "        print(f\"\\nğŸ¤– AIåŠ©æ‰‹å›å¤:\\n{'-'*40}\\n{reply}\\n{'-'*40}\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(f\"è¯·æ±‚å‡ºé”™: {resp.status_code}\")\n",
    "        return None\n",
    "\n",
    "# æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä¿®æ”¹é—®é¢˜\n",
    "ask_ai_about_image(\"/your/path/to/xxx.png\", \"å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86b093-ecf2-4942-bc23-e17930f2e538",
   "metadata": {},
   "source": [
    "### 4.2 ä½¿ç”¨OpenAIæ ¼å¼è¿›è¡Œå¯¹è¯è¯·æ±‚\n",
    "å…¶ä¸­ï¼Œapi_keyå’Œmodelå­—æ®µæ­¤å¤„æ²¡æœ‰ä½œç”¨ï¼Œä½†æ˜¯è¦å¡«ä¸Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46c94f-46f4-4934-9ed1-b9fa1807af7a",
   "metadata": {},
   "source": [
    "#### åˆå§‹åŒ–å®¢æˆ·ç«¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d043c92-dd66-4f71-a0e8-6bf6ea30b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:8080/v1',\n",
    "    api_key='not-needed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ec950-818e-4dd1-a9ed-550d935ad9e7",
   "metadata": {},
   "source": [
    "#### çº¯æ–‡æœ¬èŠå¤©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f99341-2d24-48db-983a-19547af4a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•æé—®\n",
    "response = client.chat.completions.create(\n",
    "    model='qwen3-vl',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(f\"Q: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")\n",
    "print(f\"A: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8864dc-2185-44c4-947a-673338b5252e",
   "metadata": {},
   "source": [
    "#### å¤šæ¨¡æ€--å¸¦å›¾ç‰‡çš„èŠå¤©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432457c-dd12-4e7f-aa10-475d5899cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºBase64\n",
    "image_path = \"/your/path/to/xxx.png\"  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„\n",
    "\n",
    "with open(image_path, \"rb\") as f:\n",
    "    img_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "# å‘é€æµå¼è¯·æ±‚\n",
    "response = client.chat.completions.create(\n",
    "    model='qwen3-vl',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': 'è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹'},\n",
    "            {\n",
    "                'type': 'image_url',\n",
    "                'image_url': {\n",
    "                    'url': f'data:image/jpeg;base64,{img_b64}'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }],\n",
    "    stream=True,  # å¯ç”¨æµå¼å“åº”\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"æ­£åœ¨åˆ†æå›¾ç‰‡ï¼Œæµå¼å›å¤å¼€å§‹ï¼š\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end='', flush=True)  # é€å­—æ˜¾ç¤º\n",
    "        full_response += content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf7067-a5e9-472d-a70f-3f6802000e07",
   "metadata": {},
   "source": [
    "### 4.3 ä½¿ç”¨curlè¿›è¡Œè¯·æ±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398970b-0f03-45e6-a47c-a2f1c614e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8080/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"qwen3-vl\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300,\n",
    "  \"temperature\": 0.6\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a9c75-9730-46cf-ae9e-c5d78b52784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¦å›¾ç‰‡çš„ curl\n",
    "!base64 -i /your/path/to/xxx.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548d945-c7ca-48fd-9146-b649d6fc2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8080/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"qwen3-vl\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"data:image/jpeg;base64,åœ¨è¿™é‡Œç²˜è´´ä½ åˆšåˆšå¤åˆ¶çš„å®Œæ•´Base64å­—ç¬¦ä¸²\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300,\n",
    "  \"temperature\": 0.6\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4a9e5-a391-42b9-b4f2-58acacaf73f7",
   "metadata": {},
   "source": [
    "#### å¥åº·æ£€æŸ¥ï¼š\n",
    "éšæ—¶è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œæ£€æŸ¥åŠ©æ‰‹æ˜¯å¦â€œå¥åº·åœ¨çº¿â€ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdc313-7fe8-4771-a7d9-72931d111b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:8080/health\", timeout=3)\n",
    "    print(f\"âœ… AIåŠ©æ‰‹çŠ¶æ€å¥åº· (HTTP {resp.status_code})\")\n",
    "except:\n",
    "    print(\"âŒ AIåŠ©æ‰‹æœåŠ¡æœªå“åº”ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å·²å¯åŠ¨ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ff41b-3a55-45f1-b1e9-2b7961514ff8",
   "metadata": {},
   "source": [
    "## ğŸ“š æ€»ç»“ä¸å±•æœ›\n",
    "æ­å–œä½ ï¼ğŸ‰ è‡³æ­¤ï¼Œä½ å·²ç»æˆåŠŸåœ¨æœ¬åœ°éƒ¨ç½²äº†ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¤šæ¨¡æ€AIåŠ©æ‰‹ã€‚æˆ‘ä»¬æ¥å›é¡¾ä¸€ä¸‹æ ¸å¿ƒè¦ç‚¹ï¼š\n",
    "\n",
    "- **æ ¸å¿ƒè®¤çŸ¥**ï¼šéƒ¨ç½²Qwen3-VLéœ€è¦ â€œä¸¤ä¸ªæ–‡ä»¶â€ï¼ˆä¸»æ¨¡å‹+è§†è§‰æŠ•å½±)ã€‚\n",
    "\n",
    "- **æ ¸å¿ƒæ­¥éª¤**ï¼šå‡†å¤‡æ¨¡å‹ -> ç¼–è¯‘å¼•æ“ -> å¯åŠ¨æœåŠ¡ -> å¯¹è¯æµ‹è¯•ã€‚\n",
    "\n",
    "- **å…³é”®æŠ€å·§**ï¼šåœ¨Notebookä¸­ä½¿ç”¨subprocessç®¡ç†è¿›ç¨‹ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œå¹¶å¯¹é¦–æ¬¡åŠ è½½ä¿æŒè€å¿ƒã€‚\n",
    "\n",
    "èµ„æºå¯¼èˆªï¼š\n",
    "\n",
    "- **[llama.cppå®˜æ–¹GitHub](https://github.com/ggml-org/llama.cpp)**ï¼šå…³æ³¨æ›´æ–°ï¼Œè·å–æ–‡æ¡£ã€‚\n",
    "\n",
    "- **[ModelScopeæ¨¡å‹ç¤¾åŒº](https://modelscope.cn/models)**ï¼šæµ·é‡å¼€æºæ¨¡å‹å…è´¹ä¸‹è½½ã€‚\n",
    "\n",
    "- **[Qwenå®˜æ–¹æ–‡æ¡£](https://qwen.readthedocs.io/zh-cn/latest/run_locally/llama.cpp.html)**ï¼šæ·±å…¥äº†è§£æ¨¡å‹ç‰¹æ€§ã€‚\n",
    "\n",
    "å¸Œæœ›æœ¬æ•™ç¨‹èƒ½å¸®åŠ©ä½ é¡ºåˆ©å¼€å¯æœ¬åœ°AIæ¢ç´¢ä¹‹æ—…ã€‚ç¥ä½ ç©å¾—å¼€å¿ƒï¼âœ¨\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
